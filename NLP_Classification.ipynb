{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing des données textuelles :**"
      ],
      "metadata": {
        "id": "d7TiWyWTVTJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FVbCzR24lZoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcf396b-f1c0-4241-a076-b9fd839f9743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text\n",
            "0      films adapted comic books plenty success wheth...\n",
            "1      starters created alan moore eddie campbell bro...\n",
            "2      say moore campbell thoroughly researched subje...\n",
            "3      book `` graphic novel `` 500 pages long includ...\n",
            "4                          words n't dismiss film source\n",
            "...                                                  ...\n",
            "64715    lack inspiration traced back insipid characters\n",
            "64716  like many skits current incarnation _saturday_...\n",
            "64717  watching one `` roxbury `` skits snl come away...\n",
            "64718                         bump unsuspecting women 's\n",
            "64719  watching _a_night_at_the_roxbury_ 'll left exa...\n",
            "\n",
            "[64720 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "df = pd.read_csv('movie_review.csv')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.lower() not in punctuation]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(df[['text']])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entraînement du modèle Word2Vec**"
      ],
      "metadata": {
        "id": "N-ON1CxEVbmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# texts_tokens = [text.split() for text in df['text']]\n",
        "# model = Word2Vec([texts_tokens], vector_size=100, window=5, min_count=1, sg=1)\n",
        "# print(\"exemple d'un vecteur : \" , model.wv('inspiration'))\n",
        "\n",
        "text_tokens = [text.split() for text in df['text']]\n",
        "\n",
        "\n",
        "model = Word2Vec(sentences=text_tokens,vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "\n",
        "print(\" exemple d'un vecteur d'un mot : \", model.wv['inspiration'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5K04hPCKCHi",
        "outputId": "e46db850-93d7-489d-8b04-e5f4d46feebf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " exemple d'un vecteur d'un mot :  [-0.23559007  0.40418655  0.11580639  0.21065067  0.02162356 -0.5034731\n",
            "  0.13404317  0.52630407 -0.21356523 -0.2636159  -0.11000752 -0.35239437\n",
            " -0.04663207  0.16588633 -0.09982069 -0.10590639  0.24496631 -0.18828876\n",
            " -0.04561688 -0.5620554   0.1135729   0.07947256  0.466287   -0.14504506\n",
            "  0.09519174  0.18905284 -0.14234002 -0.07610427 -0.3000506  -0.011778\n",
            "  0.29179734  0.0595244   0.04057891  0.06723082 -0.19593403  0.2853755\n",
            "  0.22905973  0.03153455 -0.09895346 -0.28737545 -0.06044609 -0.4079276\n",
            " -0.17111874 -0.09441631 -0.08158323  0.00140173 -0.11039234 -0.13458629\n",
            "  0.24698277  0.30997425  0.1099103  -0.35118636 -0.30065376 -0.09958108\n",
            " -0.03511606  0.03536397  0.1815625   0.00672357 -0.33256793 -0.0748883\n",
            "  0.10352629 -0.03633954  0.05016884 -0.17300726 -0.29831767  0.18979692\n",
            "  0.17587441  0.1454907  -0.28186393  0.4250123  -0.17378578  0.09468421\n",
            "  0.26998743 -0.08534052  0.22991167  0.11889467  0.09849129  0.15451247\n",
            " -0.2955756   0.09913653 -0.21409667 -0.03429188 -0.3066267   0.27071336\n",
            " -0.09110127 -0.08574231 -0.00942313  0.09047641  0.04645843  0.14794351\n",
            "  0.36122307  0.17384338  0.1653135  -0.0351919   0.3494541   0.12471884\n",
            "  0.27839163 -0.292124    0.12323716 -0.11868613]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorisation des reviews de movies**"
      ],
      "metadata": {
        "id": "4DKGEDATVfMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_review_vector(review, model, vector_size):\n",
        "\n",
        "    words = [word for word in review.split() if word in model.wv.key_to_index]\n",
        "    if not words:\n",
        "        return np.zeros(vector_size)\n",
        "    word_vectors = [model.wv[word] for word in words]\n",
        "    review_vector = np.mean(word_vectors, axis=0)\n",
        "    return review_vector\n",
        "\n",
        "df['vector_review'] = df['text'].apply(lambda x: get_review_vector(x, model, 100))\n",
        "\n",
        "print(df[['text', 'vector_review']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgJW0-r0MP5-",
        "outputId": "7b2b77b2-7174-4c96-cb1e-96afd029b7a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  \\\n",
            "0      films adapted comic books plenty success wheth...   \n",
            "1      starters created alan moore eddie campbell bro...   \n",
            "2      say moore campbell thoroughly researched subje...   \n",
            "3      book `` graphic novel `` 500 pages long includ...   \n",
            "4                          words n't dismiss film source   \n",
            "...                                                  ...   \n",
            "64715    lack inspiration traced back insipid characters   \n",
            "64716  like many skits current incarnation _saturday_...   \n",
            "64717  watching one `` roxbury `` skits snl come away...   \n",
            "64718                         bump unsuspecting women 's   \n",
            "64719  watching _a_night_at_the_roxbury_ 'll left exa...   \n",
            "\n",
            "                                           vector_review  \n",
            "0      [-0.2619613, 0.44805953, 0.07471598, 0.1266571...  \n",
            "1      [-0.19811782, 0.31752104, 0.06400895, 0.121749...  \n",
            "2      [-0.10082244, 0.50200546, 0.19527158, 0.084221...  \n",
            "3      [-0.31283447, 0.3323612, 0.13528155, 0.0755637...  \n",
            "4      [-0.33712667, 0.5940737, 0.050605822, 0.009082...  \n",
            "...                                                  ...  \n",
            "64715  [-0.13649341, 0.3726871, 0.0056961975, 0.08513...  \n",
            "64716  [-0.23298249, 0.41527537, 0.12980303, 0.099140...  \n",
            "64717  [-0.34178627, 0.37891105, 0.31412396, 0.003862...  \n",
            "64718  [-0.23000965, 0.36944133, 0.20365785, 0.151146...  \n",
            "64719  [-0.19606182, 0.46139717, 0.05815957, -0.02682...  \n",
            "\n",
            "[64720 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Division des données**"
      ],
      "metadata": {
        "id": "ILMNQYOCVj2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_data, test_data = train_test_split(df, test_size=0.2)\n",
        "X_train = np.vstack(train_data['vector_review'].values)\n",
        "y_train = train_data['tag']\n",
        "X_test = np.vstack(test_data['vector_review'].values)\n",
        "y_test = test_data['tag']"
      ],
      "metadata": {
        "id": "-mv8Ww52QUfp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construction d'un classificateur**"
      ],
      "metadata": {
        "id": "V2CWNgsfVny3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_predictions = logistic_model.predict(X_test)\n",
        "\n",
        "print(y_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyMMMCN6QvGj",
        "outputId": "f349739d-258b-4e44-8326-f66d6cbf8e59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pos' 'pos' 'pos' ... 'pos' 'pos' 'neg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Évaluation du modèle**"
      ],
      "metadata": {
        "id": "U7rHAZG4VtV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_predictions)\n",
        "precision = precision_score(y_test, y_predictions, pos_label='pos')\n",
        "recall = recall_score(y_test, y_predictions, pos_label='pos')\n",
        "f1 = f1_score(y_test, y_predictions, pos_label='pos')\n",
        "\n",
        "print(\"Accuracy :\", accuracy)\n",
        "print(\"Precision :\", precision)\n",
        "print(\"Recall :\", recall)\n",
        "print(\"F1-score :\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOm66QnHUzKf",
        "outputId": "27ea9aad-f2e6-4d93-bc6d-b995df220c5a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.5801143386897404\n",
            "Precision : 0.5819773006973882\n",
            "Recall : 0.6415435634609586\n",
            "F1-score : 0.610310461031046\n"
          ]
        }
      ]
    }
  ]
}